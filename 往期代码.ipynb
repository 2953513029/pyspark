{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a950f3ab-583c-4e11-b4f2-59a3fdabebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import column\n",
    "#添加注释#\n",
    "conf=SparkConf().setAppName(\"test\").setMaster(\"local[*]\")\n",
    "sc=SparkContext.getOrCreate(conf)\n",
    "spark=SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "795eeaec-3385-4346-9194-f0d96042eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName('text').master(\"local\").getOrCreate()\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28fd5120",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = spark.read.option('header','true').option('inferSchema','true').option('encoding','gbk').option('delimiter',',').csv('D:\\Pyspark\\code/air_data_customer.csv')\n",
    "f = spark.read.option('header','true').option('inferSchema','true').option('encoding','gbk').option('delimiter',',').csv('D:\\Pyspark\\code/air_data_flight.csv')\n",
    "p = spark.read.option('header','true').option('inferSchema','true').option('encoding','gbk').option('delimiter',',').csv('D:\\Pyspark\\code/air_data_points.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "905b9359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------+--------+-------------+------------+---+\n",
      "|MEMBER_NO| FFP_DATE|GENDER|FFP_TIER|WORK_PROVINCE|WORK_COUNTRY|AGE|\n",
      "+---------+---------+------+--------+-------------+------------+---+\n",
      "|    54993|2006/11/2|    男|       6|         北京|          CN| 31|\n",
      "|    28065|2007/2/19|    男|       6|         北京|          CN| 42|\n",
      "|    55106| 2007/2/1|    男|       6|         北京|          CN| 40|\n",
      "+---------+---------+------+--------+-------------+------------+---+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+-----------------+---------+------------+----------+----------------+--------+--------+-----------+------------+------------+------------+\n",
      "|MEMBER_NO|FIRST_FLIGHT_DATE|LOAD_TIME|FLIGHT_COUNT|SEG_KM_SUM|LAST_FLIGHT_DATE|SUM_YR_1|SUM_YR_2|LAST_TO_END|AVG_DISCOUNT|AVG_INTERVAL|MAX_INTERVAL|\n",
      "+---------+-----------------+---------+------------+----------+----------------+--------+--------+-----------+------------+------------+------------+\n",
      "|    54993|       2008/12/24|2014/3/31|         210|    580717|       2014/3/31|239560.0|  234188|          1| 0.961639043| 3.483253589|          18|\n",
      "|    28065|         2007/8/3|2014/3/31|         140|    293678|       2014/3/25|171483.0|  167434|          7|  1.25231444| 5.194244604|          17|\n",
      "|    55106|        2007/8/30|2014/3/31|         135|    283712|       2014/3/21|163618.0|  164982|         11| 1.254675516| 5.298507463|          18|\n",
      "+---------+-----------------+---------+------------+----------+----------------+--------+--------+-----------+------------+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+--------------+------+------+----------+--------------+----------+\n",
      "|MEMBER_NO|EXCHANGE_COUNT|EP_SUM|BP_SUM|AVG_BP_SUM|BEGIN_TO_FIRST|Points_Sum|\n",
      "+---------+--------------+------+------+----------+--------------+----------+\n",
      "|    54993|            34| 74460|505308|   63163.5|             2|    619760|\n",
      "|    28065|            29| 41288|362480|   45310.0|             2|    415768|\n",
      "|    55106|            20| 39711|351159| 43894.875|            10|    406361|\n",
      "+---------+--------------+------+------+----------+--------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.show(3)\n",
    "f.show(3)\n",
    "p.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d45c3712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------+---+\n",
      "| id|                name|gender|age|\n",
      "+---+--------------------+------+---+\n",
      "|  1|Zhangsan            |  M   | 19|\n",
      "|  2|Limei               |  F   | 19|\n",
      "+---+--------------------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jdbcdf = spark.read.format('jdbc')\\\n",
    "    .option('driver', 'com.mysql.cj.jdbc.Driver')\\\n",
    "    .option('url', 'jdbc:mysql://localhost:3306/test?useUnicode=true&characterEncoding=UTF-8&serverTimezone=UTC&useSSL=false&allowPublicKeyRetrieval=true')\\\n",
    "    .option('dbtable', 'student')\\\n",
    "    .option('user', 'root')\\\n",
    "    .option('password', '123456')\\\n",
    "    .load()\n",
    "\n",
    "jdbcdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34373294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------+---+\n",
      "| id|                name|gender|age|\n",
      "+---+--------------------+------+---+\n",
      "|  1|Zhangsan            |  M   | 19|\n",
      "|  2|Limei               |  F   | 19|\n",
      "+---+--------------------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jdbcdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4353a65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----+\n",
      "|    name|age|score|\n",
      "+--------+---+-----+\n",
      "|zhangsan| 18|  588|\n",
      "|    Lisi| 18|  590|\n",
      "|  wangwu| 17|  560|\n",
      "+--------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Listrdd=sc.parallelize([('zhangsan',18,588),('Lisi',18,590),('wangwu',17,560)])\n",
    "Listdf=Listrdd.toDF(['name','age','score'])\n",
    "Listdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3b6493c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\Pyspark\\Anaconda3\\envs\\pyspark\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Pyspark\\Anaconda3\\envs\\pyspark\\Lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Pyspark\\Anaconda3\\envs\\pyspark\\Lib\\socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m values = [(\u001b[33m'\u001b[39m\u001b[33mZhangSan\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m18\u001b[39m), (\u001b[33m'\u001b[39m\u001b[33mLisi\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m18\u001b[39m), (\u001b[33m'\u001b[39m\u001b[33mWangWu\u001b[39m\u001b[33m'\u001b[39m, \u001b[32m17\u001b[39m)]\n\u001b[32m      2\u001b[39m Listdf2 = spark.createDataFrame(values, [\u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mage\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mListdf2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Pyspark\\Anaconda3\\envs\\pyspark\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[39m, in \u001b[36mDataFrame.show\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    887\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m = \u001b[32m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    888\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[32m    889\u001b[39m \n\u001b[32m    890\u001b[39m \u001b[33;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    945\u001b[39m \u001b[33;03m    name | Bob\u001b[39;00m\n\u001b[32m    946\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Pyspark\\Anaconda3\\envs\\pyspark\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:965\u001b[39m, in \u001b[36mDataFrame._show_string\u001b[39m\u001b[34m(self, n, truncate, vertical)\u001b[39m\n\u001b[32m    959\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m    960\u001b[39m         error_class=\u001b[33m\"\u001b[39m\u001b[33mNOT_BOOL\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    961\u001b[39m         message_parameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mvertical\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical).\u001b[34m__name__\u001b[39m},\n\u001b[32m    962\u001b[39m     )\n\u001b[32m    964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    967\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Pyspark\\Anaconda3\\envs\\pyspark\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Pyspark\\Anaconda3\\envs\\pyspark\\Lib\\site-packages\\py4j\\java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Pyspark\\Anaconda3\\envs\\pyspark\\Lib\\site-packages\\py4j\\clientserver.py:511\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m         answer = smart_decode(\u001b[38;5;28mself\u001b[39m.stream.readline()[:-\u001b[32m1\u001b[39m])\n\u001b[32m    512\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    513\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Pyspark\\Anaconda3\\envs\\pyspark\\Lib\\socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    720\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "values = [('ZhangSan', 18), ('Lisi', 18), ('WangWu', 17)]\n",
    "Listdf2 = spark.createDataFrame(values, ['name', 'age'])\n",
    "Listdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed3c8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|张三| 18|\n",
      "|李四| 18|\n",
      "|王五| 17|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pdf = pd.DataFrame([('张三', 18), ('李四', 18), ('王五', 17)], columns=['name', 'age'])\n",
    "pandasdf = spark.createDataFrame(pdf)\n",
    "pandasdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58cfc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----------+\n",
      "|    name|score|  birthday|\n",
      "+--------+-----+----------+\n",
      "|ZhangSan|  588|2013-01-15|\n",
      "|    Lisi|  599|2013-05-12|\n",
      "|  WangWu|  560|2014-07-23|\n",
      "+--------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "\n",
    "schema = StructType([StructField('name', StringType(), nullable=False), \\\n",
    "    StructField('score', IntegerType(), nullable=True), \\\n",
    "    StructField('birthday', DateType(), nullable=True)])\n",
    "\n",
    "rdd = sc.parallelize([Row('ZhangSan', 588, datetime(2013, 1, 15)), \\\n",
    "    Row('Lisi', 599, datetime(2013, 5, 12)), \\\n",
    "    Row('WangWu', 560, datetime(2014, 7, 23))])\n",
    "\n",
    "df4 = spark.createDataFrame(rdd, schema)\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b737e6bf",
   "metadata": {},
   "source": [
    "分割线————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
