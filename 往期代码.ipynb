{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a950f3ab-583c-4e11-b4f2-59a3fdabebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import column\n",
    "\n",
    "conf=SparkConf().setAppName(\"test\").setMaster(\"local[*]\")\n",
    "sc=SparkContext.getOrCreate(conf)\n",
    "spark=SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "795eeaec-3385-4346-9194-f0d96042eecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession.builder.appName('text').master(\"local\").getOrCreate()\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28fd5120",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = spark.read.option('header','true').option('inferSchema','true').option('encoding','gbk').option('delimiter',',').csv('D:\\Pyspark\\code/air_data_customer.csv')\n",
    "f = spark.read.option('header','true').option('inferSchema','true').option('encoding','gbk').option('delimiter',',').csv('D:\\Pyspark\\code/air_data_flight.csv')\n",
    "p = spark.read.option('header','true').option('inferSchema','true').option('encoding','gbk').option('delimiter',',').csv('D:\\Pyspark\\code/air_data_points.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "905b9359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+------+--------+-------------+------------+---+\n",
      "|MEMBER_NO| FFP_DATE|GENDER|FFP_TIER|WORK_PROVINCE|WORK_COUNTRY|AGE|\n",
      "+---------+---------+------+--------+-------------+------------+---+\n",
      "|    54993|2006/11/2|    男|       6|         北京|          CN| 31|\n",
      "|    28065|2007/2/19|    男|       6|         北京|          CN| 42|\n",
      "|    55106| 2007/2/1|    男|       6|         北京|          CN| 40|\n",
      "+---------+---------+------+--------+-------------+------------+---+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+-----------------+---------+------------+----------+----------------+--------+--------+-----------+------------+------------+------------+\n",
      "|MEMBER_NO|FIRST_FLIGHT_DATE|LOAD_TIME|FLIGHT_COUNT|SEG_KM_SUM|LAST_FLIGHT_DATE|SUM_YR_1|SUM_YR_2|LAST_TO_END|AVG_DISCOUNT|AVG_INTERVAL|MAX_INTERVAL|\n",
      "+---------+-----------------+---------+------------+----------+----------------+--------+--------+-----------+------------+------------+------------+\n",
      "|    54993|       2008/12/24|2014/3/31|         210|    580717|       2014/3/31|239560.0|  234188|          1| 0.961639043| 3.483253589|          18|\n",
      "|    28065|         2007/8/3|2014/3/31|         140|    293678|       2014/3/25|171483.0|  167434|          7|  1.25231444| 5.194244604|          17|\n",
      "|    55106|        2007/8/30|2014/3/31|         135|    283712|       2014/3/21|163618.0|  164982|         11| 1.254675516| 5.298507463|          18|\n",
      "+---------+-----------------+---------+------------+----------+----------------+--------+--------+-----------+------------+------------+------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+---------+--------------+------+------+----------+--------------+----------+\n",
      "|MEMBER_NO|EXCHANGE_COUNT|EP_SUM|BP_SUM|AVG_BP_SUM|BEGIN_TO_FIRST|Points_Sum|\n",
      "+---------+--------------+------+------+----------+--------------+----------+\n",
      "|    54993|            34| 74460|505308|   63163.5|             2|    619760|\n",
      "|    28065|            29| 41288|362480|   45310.0|             2|    415768|\n",
      "|    55106|            20| 39711|351159| 43894.875|            10|    406361|\n",
      "+---------+--------------+------+------+----------+--------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.show(3)\n",
    "f.show(3)\n",
    "p.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d45c3712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------+---+\n",
      "| id|                name|gender|age|\n",
      "+---+--------------------+------+---+\n",
      "|  1|Zhangsan            |  M   | 19|\n",
      "|  2|Limei               |  F   | 19|\n",
      "+---+--------------------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jdbcdf = spark.read.format('jdbc')\\\n",
    "    .option('driver', 'com.mysql.cj.jdbc.Driver')\\\n",
    "    .option('url', 'jdbc:mysql://localhost:3306/test?useUnicode=true&characterEncoding=UTF-8&serverTimezone=UTC&useSSL=false&allowPublicKeyRetrieval=true')\\\n",
    "    .option('dbtable', 'student')\\\n",
    "    .option('user', 'root')\\\n",
    "    .option('password', '123456')\\\n",
    "    .load()\n",
    "\n",
    "jdbcdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34373294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+------+---+\n",
      "| id|                name|gender|age|\n",
      "+---+--------------------+------+---+\n",
      "|  1|Zhangsan            |  M   | 19|\n",
      "|  2|Limei               |  F   | 19|\n",
      "+---+--------------------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jdbcdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4353a65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+-----+\n",
      "|    name|age|score|\n",
      "+--------+---+-----+\n",
      "|zhangsan| 18|  588|\n",
      "|    Lisi| 18|  590|\n",
      "|  wangwu| 17|  560|\n",
      "+--------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Listrdd=sc.parallelize([('zhangsan',18,588),('Lisi',18,590),('wangwu',17,560)])\n",
    "Listdf=Listrdd.toDF(['name','age','score'])\n",
    "Listdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3b6493c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+\n",
      "|    name|age|\n",
      "+--------+---+\n",
      "|ZhangSan| 18|\n",
      "|    Lisi| 18|\n",
      "|  WangWu| 17|\n",
      "+--------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "values = [('ZhangSan', 18), ('Lisi', 18), ('WangWu', 17)]\n",
    "Listdf2 = spark.createDataFrame(values, ['name', 'age'])\n",
    "Listdf2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ed3c8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|name|age|\n",
      "+----+---+\n",
      "|张三| 18|\n",
      "|李四| 18|\n",
      "|王五| 17|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pdf = pd.DataFrame([('张三', 18), ('李四', 18), ('王五', 17)], columns=['name', 'age'])\n",
    "pandasdf = spark.createDataFrame(pdf)\n",
    "pandasdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c58cfc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+----------+\n",
      "|    name|score|  birthday|\n",
      "+--------+-----+----------+\n",
      "|ZhangSan|  588|2013-01-15|\n",
      "|    Lisi|  599|2013-05-12|\n",
      "|  WangWu|  560|2014-07-23|\n",
      "+--------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from datetime import datetime\n",
    "\n",
    "schema = StructType([StructField('name', StringType(), nullable=False), \\\n",
    "    StructField('score', IntegerType(), nullable=True), \\\n",
    "    StructField('birthday', DateType(), nullable=True)])\n",
    "\n",
    "rdd = sc.parallelize([Row('ZhangSan', 588, datetime(2013, 1, 15)), \\\n",
    "    Row('Lisi', 599, datetime(2013, 5, 12)), \\\n",
    "    Row('WangWu', 560, datetime(2014, 7, 23))])\n",
    "\n",
    "df4 = spark.createDataFrame(rdd, schema)\n",
    "df4.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
